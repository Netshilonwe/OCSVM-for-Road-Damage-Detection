{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a132a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.svm import OneClassSVM\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Specify the directory containing the normal class images\n",
    "normal_class_directory = \"C:/Users/vnets/PycharmProjects/pythonProject/Training\"\n",
    "\n",
    "# Initialize an empty list to store the image data\n",
    "X_train_normal = []\n",
    "\n",
    "# Iterate over each file in the normal class directory\n",
    "for filename in os.listdir(normal_class_directory):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        img_path = os.path.join(normal_class_directory, filename)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None and img.size > 0:\n",
    "            img_resized = cv2.resize(img, (500, 500))\n",
    "            edges = cv2.Canny(img_resized, threshold1=100, threshold2=200)\n",
    "            X_train_normal.append(edges)\n",
    "\n",
    "# Convert the list to a numpy array\n",
    "X_train_normal = np.array(X_train_normal)\n",
    "X_train_normal = X_train_normal.reshape(len(X_train_normal), -1)\n",
    "X_train_normal = X_train_normal.astype('float32') / 255.0\n",
    "\n",
    "# Train the One-Class SVM model\n",
    "ocsvm = OneClassSVM(kernel='rbf', gamma='scale')\n",
    "ocsvm.fit(X_train_normal)\n",
    "\n",
    "# Save the trained One-Class SVM model\n",
    "model_filename = os.path.join(normal_class_directory, 'ocsvm_model.pkl')\n",
    "joblib.dump(ocsvm, model_filename)\n",
    "print(f\"Model saved as {model_filename}\")\n",
    "\n",
    "print(\"Training is Finished\")\n",
    "\n",
    "# Load the trained One-Class SVM model\n",
    "loaded_ocsvm = joblib.load(model_filename)\n",
    "\n",
    "# Specify the directory containing the anomaly class images for validation\n",
    "anomaly_val_directory = \"C:/Users/vnets/PycharmProjects/pythonProject/Validation\"\n",
    "\n",
    "# Prepare and preprocess the validation data (anomaly class)\n",
    "X_val_anomaly = []\n",
    "\n",
    "for filename in os.listdir(anomaly_val_directory):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        img_path = os.path.join(anomaly_val_directory, filename)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None and img.size > 0:\n",
    "            img_resized = cv2.resize(img, (500, 500))\n",
    "            edges = cv2.Canny(img_resized, threshold1=100, threshold2=200)\n",
    "            X_val_anomaly.append(edges)\n",
    "\n",
    "X_val_anomaly = np.array(X_val_anomaly)\n",
    "X_val_anomaly = X_val_anomaly.reshape(len(X_val_anomaly), -1)\n",
    "X_val_anomaly = X_val_anomaly.astype('float32') / 255.0\n",
    "\n",
    "# Predict anomalies on the validation set using the loaded model\n",
    "val_anomaly_predictions = loaded_ocsvm.predict(X_val_anomaly)\n",
    "\n",
    "# Specify the directory containing the anomaly class images for testing\n",
    "anomaly_test_directory = \"C:/Users/vnets/PycharmProjects/pythonProject/Testing\"\n",
    "\n",
    "# Prepare and preprocess the test data (anomaly class)\n",
    "X_test_anomaly = []\n",
    "\n",
    "for filename in os.listdir(anomaly_test_directory):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        img_path = os.path.join(anomaly_test_directory, filename)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None and img.size > 0:\n",
    "            img_resized = cv2.resize(img, (500, 500))\n",
    "            edges = cv2.Canny(img_resized, threshold1=100, threshold2=200)\n",
    "            X_test_anomaly.append(edges)\n",
    "\n",
    "X_test_anomaly = np.array(X_test_anomaly)\n",
    "X_test_anomaly = X_test_anomaly.reshape(len(X_test_anomaly), -1)\n",
    "X_test_anomaly = X_test_anomaly.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138f89e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict anomalies on the testing set using the loaded model\n",
    "test_anomaly_predictions = loaded_ocsvm.predict(X_test_anomaly)\n",
    "\n",
    "#Print the validation set anomaly predictions\n",
    "print(\"Validation Set Anomaly Predictions:\")\n",
    "print(\" \".join(map(str, val_anomaly_predictions)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb958585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Seaborn visualization for validation set\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(val_anomaly_predictions, bins=15, kde=True, color='blue', label='Validation Set')\n",
    "plt.title(\"Validation Set Anomaly Predictions Distribution\")\n",
    "plt.xlabel(\"Prediction Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a657a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the testing set anomaly predictions\n",
    "print(\"Testing Set Anomaly Predictions:\")\n",
    "print(\" \".join(map(str, test_anomaly_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cc85af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Seaborn visualization for testing set\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(test_anomaly_predictions, bins=15, kde=True, color='orange', label='Testing Set')\n",
    "plt.title(\"Testing Set Anomaly Predictions Distribution\")\n",
    "plt.xlabel(\"Prediction Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d128d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform k-fold cross-validation and get the scores\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation and get the scores\n",
    "cv_scores = cross_val_score(ocsvm, X_val_anomaly, cv=kfold, scoring='accuracy')  # Adjust scoring metric if needed\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean accuracy:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49113e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference (loss) between validation and testing predictions\n",
    "loss = abs(val_anomaly_predictions + test_anomaly_predictions)\n",
    "\n",
    "# Plot the loss\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(loss, bins=20, kde=True, color='blue')\n",
    "plt.title('Difference (Loss) between Validation and Testing Predictions')\n",
    "plt.xlabel('Loss')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b61d50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a trained OC-SVM model named 'ocsvm_model'\n",
    "# Assuming you have test data named 'X_test_anomaly' and true labels for test data named 'true_labels'\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Get the predictions from the OC-SVM model\n",
    "test_predictions = ocsvm.predict(X_test_anomaly)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(test_anomaly_predictions,test_predictions)\n",
    "\n",
    "# Create a Seaborn heatmap for the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"Anomaly\", \"Normal\"],\n",
    "            yticklabels=[\"Anomaly\", \"Normal\"])\n",
    "plt.title(\"Confusion Matrix for OC-SVM Predictions\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1002f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have the data: val_anomaly_predictions and test_anomaly_predictions\n",
    "\n",
    "# Plot a histogram of predicted labels for the validation set\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(val_anomaly_predictions, bins=5, color='blue', alpha=0.7, histtype='bar', edgecolor='black')  # Use 'bar' type\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Predicted Labels for Validation Set')\n",
    "plt.xticks([-1, 1], ['Anomaly', 'Normal'])\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()\n",
    "\n",
    "# Plot a histogram of predicted labels for the testing set\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(test_anomaly_predictions, bins=5, color='green', alpha=0.7, histtype='bar', edgecolor='black')  # Use 'bar' type\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Predicted Labels for Testing Set')\n",
    "plt.xticks([-1, 1], ['Anomaly', 'Normal'])\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbc4eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319595f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the components of the custom \"confusion matrix\"\n",
    "true_anomalies = np.sum(test_anomaly_predictions == -1)\n",
    "true_positives = np.sum((test_anomaly_predictions == -1) & (test_predictions == -1))\n",
    "false_alarms = np.sum((test_anomaly_predictions == 1) & (test_predictions == -1))\n",
    "missed_anomalies = true_anomalies - true_positives\n",
    "true_normals = np.sum(test_anomaly_predictions == 1)\n",
    "\n",
    "# Display the custom \"confusion matrix\"\n",
    "print(\"Custom Confusion Matrix:\")\n",
    "print(f\"True Positives (TP): {true_positives}\")\n",
    "print(f\"False Positives: {true_normals}\")\n",
    "print(f\"Missed Anomalies: {missed_anomalies}\")\n",
    "print(f\"True Normal: {false_alarms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f25ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have calculated the components of the custom \"confusion matrix\"\n",
    "true_positives = 300\n",
    "false_alarms = 21\n",
    "missed_anomalies = 0\n",
    "true_normals = 0\n",
    "\n",
    "# Creating a custom \"confusion matrix\" as a numpy array\n",
    "custom_confusion = np.array([[true_positives, false_alarms],\n",
    "                             [missed_anomalies, true_normals]])\n",
    "\n",
    "# Creating a custom heatmap-like visualization\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(custom_confusion, cmap='Blues', interpolation='nearest')\n",
    "plt.colorbar(label='Count')\n",
    "plt.xticks([0, 1], ['Anomalies', 'Normal'])\n",
    "plt.yticks([0, 1], ['Anomalies', 'Normal'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix Visualization')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33211a88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "# Get the decision function values from the OC-SVM model\n",
    "decision_values = ocsvm.decision_function(X_test_anomaly)\n",
    "\n",
    "# For one-class ROC curve, label '-1' means anomalies (damaged road instances)\n",
    "y_true = -1 * np.ones(len(decision_values))\n",
    "\n",
    "# Calculate the ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_true, decision_values)\n",
    "\n",
    "# Calculate the AUC score\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cde2c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Manually calculated precision and recall values\n",
    "manual_precision = [0.85, 0.75, 0.65]  # Replace with your values\n",
    "manual_recall = [0.80, 0.65, 0.50]  # Replace with your values\n",
    "\n",
    "# Plot the Precision-Recall curve\n",
    "plt.plot(manual_recall, manual_precision, marker='o', linestyle='-')\n",
    "\n",
    "# Mark specific points\n",
    "plt.scatter(0.5, 0.75, color='r', label='Recall=0.5, Precision=0.75')\n",
    "plt.scatter(0.7, 0.85, color='g', label='Recall=0.7, Precision=0.85')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-Recall curve')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a762d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "# Get the decision function values from the OC-SVM model\n",
    "decision_values = ocsvm.decision_function(X_test_anomaly)\n",
    "\n",
    "# For precision-recall curve, label '-1' means anomalies (damaged road instances)\n",
    "y_true = -1 * np.ones(len(decision_values))\n",
    "\n",
    "# Calculate the precision-recall curve\n",
    "precision, recall, _ = precision_recall_curve(y_true, decision_values)\n",
    "\n",
    "# Calculate the AUC-PR score\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "# Plot the precision-recall curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, color='darkorange', lw=2, label='PR curve (area = %0.2f)' % pr_auc)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec624df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "\n",
    "# Get the decision function values from the OC-SVM model\n",
    "decision_values = ocsvm.decision_function(X_test_anomaly)\n",
    "\n",
    "# Normalize decision values to [0, 1] range\n",
    "normalized_decision_values = (decision_values - decision_values.min()) / (decision_values.max() - decision_values.min())\n",
    "\n",
    "# Define threshold values for different damage levels\n",
    "low_damage_threshold = 0.2\n",
    "medium_damage_threshold = 0.5\n",
    "\n",
    "# Categorize levels of damage\n",
    "damage_levels = np.where(normalized_decision_values < low_damage_threshold, 'Low Damage',\n",
    "                         np.where(normalized_decision_values < medium_damage_threshold, 'Medium Damage', 'High Damage'))\n",
    "\n",
    "# Now 'damage_levels' contains the categorization of damage levels for each data point\n",
    "\n",
    "# Print damage levels for debugging\n",
    "print(\"Damage Levels:\", damage_levels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77ee353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Map damage levels to colors\n",
    "damage_colors = {\n",
    "    'Low Damage': 'green',\n",
    "    'Medium Damage': 'orange',\n",
    "    'High Damage': 'red'\n",
    "}\n",
    "\n",
    "# Define the number of rows and columns for the grid\n",
    "num_rows = 7\n",
    "num_cols = 3\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i in range(num_rows * num_cols):\n",
    "    img = cv2.imread(os.path.join(anomaly_test_directory, os.listdir(anomaly_test_directory)[i]))\n",
    "    damage_level = damage_levels[i]\n",
    "\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    plt.subplot(num_rows, num_cols, i + 1)\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.title(f\"Damage: {damage_level}\", color=damage_colors[damage_level])\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea710f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print images that were predicted as anomalies (with prediction value 1)\n",
    "num_images_to_display =12   # Number of images to display\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "displayed_images = 0\n",
    "for idx, prediction in enumerate(test_anomaly_predictions):\n",
    "    if prediction == 1:\n",
    "        img = cv2.imread(os.path.join(anomaly_test_directory, os.listdir(anomaly_test_directory)[idx]))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        plt.subplot(num_rows, num_cols, displayed_images + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Prediction: {prediction}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        displayed_images += 1\n",
    "        if displayed_images == num_images_to_display:\n",
    "            break\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7a5c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print images that were predicted as anomalies (with prediction value 1)\n",
    "num_images_to_display = 9  # Number of images to display\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "displayed_images = 0\n",
    "for idx, prediction in enumerate(test_anomaly_predictions):\n",
    "    if prediction == -1:\n",
    "        img = cv2.imread(os.path.join(anomaly_test_directory, os.listdir(anomaly_test_directory)[idx]))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        plt.subplot(num_rows, num_cols, displayed_images + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Prediction: {prediction}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        displayed_images += 1\n",
    "        if displayed_images == num_images_to_display:\n",
    "            break\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0870ccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Assuming 'damage_levels' is already calculated as in your previous code\n",
    "\n",
    "# Map damage levels to colors\n",
    "damage_colors = {\n",
    "    'Low Damage': 'green',\n",
    "    'Medium Damage': 'orange',\n",
    "    'High Damage': 'red'\n",
    "}\n",
    "\n",
    "# Define the number of rows and columns for the grid\n",
    "num_rows = 7\n",
    "num_cols = 3\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Dictionary to store indices for each damage level\n",
    "damage_indices = {\n",
    "    'Low Damage': np.where(damage_levels == 'Low Damage')[0],\n",
    "    'Medium Damage': np.where(damage_levels == 'Medium Damage')[0],\n",
    "    'High Damage': np.where(damage_levels == 'High Damage')[0]\n",
    "}\n",
    "\n",
    "for damage_level, indices in damage_indices.items():\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        img = cv2.imread(os.path.join(anomaly_test_directory, os.listdir(anomaly_test_directory)[idx]))\n",
    "\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        plt.subplot(num_rows, num_cols, i + 1)\n",
    "        plt.imshow(img_rgb)\n",
    "        plt.title(f\"Damage: {damage_level}\", color=damage_colors[damage_level])\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b598b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate some example model confidence scores (replace with actual scores)\n",
    "model_confidence_scores = np.random.rand(100)\n",
    "\n",
    "# Plot histogram of model confidence scores\n",
    "plt.hist(model_confidence_scores, bins=20, alpha=0.5, label='Model Confidence Scores')\n",
    "\n",
    "plt.xlabel('Model Confidence Scores')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.title('Histogram of Model Confidence Scores')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d7e40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CROSS VALIDATION\n",
    "import joblib\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# Load the trained One-Class SVM model\n",
    "loaded_ocsvm = joblib.load(model_filename)\n",
    "\n",
    "# Specify the directory containing the anomaly class images for validation\n",
    "Cross_Val = \"C:/Users/vnets/PycharmProjects/pythonProject/Cross\"\n",
    "\n",
    "# Prepare and preprocess the validation data (anomaly class)\n",
    "X = []\n",
    "\n",
    "for filename in os.listdir(anomaly_val_directory):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        img_path = os.path.join(anomaly_val_directory, filename)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None and img.size > 0:\n",
    "            img_resized = cv2.resize(img, (500, 500))\n",
    "            edges = cv2.Canny(img_resized, threshold1=100, threshold2=200)\n",
    "            X.append(edges)\n",
    "\n",
    "X = np.array(X)\n",
    "X= X_val_anomaly.reshape(len(X), -1)\n",
    "X_val_anomaly = X.astype('float32') / 255.0\n",
    "\n",
    "# Predict anomalies on the validation set using the loaded model\n",
    "val_anomaly_predictions = loaded_ocsvm.predict(X)\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "num_folds = 5  # You can choose a different number of folds\n",
    "\n",
    "# Initialize k-fold cross-validation\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation and get the scores\n",
    "cv_scores = cross_val_score(ocsvm_model, X, cv=kfold, scoring='accuracy')  # Adjust scoring metric if needed\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean accuracy:\", cv_scores.mean())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8494d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
